\hypertarget{namespaceaipy_1_1aipy_1_1utils}{}\doxysection{aipy.\+aipy.\+utils Namespace Reference}
\label{namespaceaipy_1_1aipy_1_1utils}\index{aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_adc552a252a7228b685a589a2778c9229}{initialize\+\_\+with\+\_\+zeros}} (dim)
\begin{DoxyCompactList}\small\item\em This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_aa064d8aa342cee14bd78510b0e607860}{linear}} (w, X, b)
\begin{DoxyCompactList}\small\item\em linear activation function \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_ab953b6a76b3d91de0f6b42d9d6d8b58b}{sigmoid}} (z)
\begin{DoxyCompactList}\small\item\em sigmoid activation \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a21734b28fb87a0c8494e865442d9b4ec}{deep\+\_\+sigmoid}} (Z)
\begin{DoxyCompactList}\small\item\em Implements the sigmoid activation in numpy. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a069feeb21ef242f65007f62e77509639}{compute\+\_\+cost}} (Y, h)
\begin{DoxyCompactList}\small\item\em compute log likelihood of the logistic regression \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_ac1d2386f78b74fda92f3eedb0690688d}{deep\+\_\+compute\+\_\+cost}} (AL, Y)
\begin{DoxyCompactList}\small\item\em Implement the cost function defined by equation (7). \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a87e9e817af292c366226f644168454a7}{derive\+\_\+weight}} (X, Y, h)
\begin{DoxyCompactList}\small\item\em compute d\+J/dw, and d\+J/db for logistic regression \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a757ce3a23c10fbc489f586d772381d68}{update\+\_\+weight}} (w, dw, b, db, alpha)
\begin{DoxyCompactList}\small\item\em Update weight values with single gradient descent step. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_af4b8d65cdcdd5476f1fd2c871464c391}{update\+\_\+parameters}} (parameters, grads, learning\+\_\+rate)
\begin{DoxyCompactList}\small\item\em Update parameters using gradient descent. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a58e2fccbc2a5b25f8f30607629ed38ae}{propagate}} (w, b, X, Y)
\begin{DoxyCompactList}\small\item\em Implement Forward, and Backward propagation, the cost function and its gradient . \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a96dbd24dcc5f813264a1bc91be208946}{gradient\+\_\+descent}} (w, b, X, Y, num\+\_\+iterations=1000, alpha=0.\+001, print\+\_\+cost=False)
\begin{DoxyCompactList}\small\item\em This function optimizes w and b by running a gradient descent algorithm. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a88d9dbe6880b5c7ce1978a290b21b0db}{predict}} (w, b, X)
\begin{DoxyCompactList}\small\item\em Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a314bb57a2c296337ea2f273c46f4e7e7}{relu}} (Z)
\begin{DoxyCompactList}\small\item\em Implement the R\+E\+LU function. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_ae93689ee55421e957e55192016219da0}{relu\+\_\+backward}} (dA, cache)
\begin{DoxyCompactList}\small\item\em Implement the backward propagation for a single R\+E\+LU unit. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_aec2b9988afa351577eb52f731029fc89}{sigmoid\+\_\+backward}} (dA, cache)
\begin{DoxyCompactList}\small\item\em Implement the backward propagation for a single S\+I\+G\+M\+O\+ID unit. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_ac5ddc42640b585e150188f5afba6dfe7}{initialize\+\_\+parameters}} (n\+\_\+x, n\+\_\+h, n\+\_\+y)
\begin{DoxyCompactList}\small\item\em Randomly initialize the parameters W, b. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_afbac86750318632038fd04f8ad723177}{linear\+\_\+forward}} (A, W, b)
\begin{DoxyCompactList}\small\item\em Implement the linear part of a layer\textquotesingle{}s forward propagation. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a08062c4331864f05b91e3e6c385996a5}{forward\+\_\+propagation}} (X, parameters)
\begin{DoxyCompactList}\small\item\em Implement Forward propagation. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_ac22409ca5bf4e2a84069b8832259aa2a}{deep\+\_\+initialization}} (layer\+\_\+dims)
\begin{DoxyCompactList}\small\item\em deep neural networks \# \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_abd0b296a51a42b8841de2e0ec584e21c}{deep\+\_\+linear}} (W, A, b)
\begin{DoxyCompactList}\small\item\em linear activation function \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_ae772d750fa98fd997ee0f20e2e255f72}{deep\+\_\+activation}} (z)
\begin{DoxyCompactList}\small\item\em Tan activation function. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a9053ccc44ee85c2b505ca8235b264ba4}{deep\+\_\+linear\+\_\+activation\+\_\+forward}} (A\+\_\+prev, W, b, activation)
\begin{DoxyCompactList}\small\item\em Implement the forward propagation for the L\+I\+N\+E\+A\+R-\/$>$A\+C\+T\+I\+V\+A\+T\+I\+ON layer. \end{DoxyCompactList}\item 
def \mbox{\hyperlink{namespaceaipy_1_1aipy_1_1utils_a57e2d187ebc733e018a0dcf6a506af9a}{deep\+\_\+linear\+\_\+activation\+\_\+backward}} (dA, cache, activation)
\begin{DoxyCompactList}\small\item\em Implement the backward propagation for the L\+I\+N\+E\+A\+R-\/$>$A\+C\+T\+I\+V\+A\+T\+I\+ON layer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}@package aipy

this is utility file for aipy library
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a069feeb21ef242f65007f62e77509639}\label{namespaceaipy_1_1aipy_1_1utils_a069feeb21ef242f65007f62e77509639}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!compute\_cost@{compute\_cost}}
\index{compute\_cost@{compute\_cost}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{compute\_cost()}{compute\_cost()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+compute\+\_\+cost (\begin{DoxyParamCaption}\item[{}]{Y,  }\item[{}]{h }\end{DoxyParamCaption})}



compute log likelihood of the logistic regression 


\begin{DoxyParams}{Parameters}
{\em Y} & (1,m) labeled vector Output \\
\hline
{\em h} & (1,m) estimated output \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
J (1) scalar cost function output 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00051}{51}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00051 \textcolor{keyword}{def }compute\_cost(Y, h):}
\DoxyCodeLine{00052     m=Y.shape[1]}
\DoxyCodeLine{00053     \textcolor{keyword}{def }compute\_loss():}
\DoxyCodeLine{00054         L=np.dot(Y.T, np.log(h)) + np.dot((1-\/Y).T, np.log((1-\/h)))}
\DoxyCodeLine{00055         \textcolor{keywordflow}{return} L.squeeze()}
\DoxyCodeLine{00056     J = -\/1.0/m * compute\_loss()}
\DoxyCodeLine{00057     \textcolor{keywordflow}{return} J}
\DoxyCodeLine{00058 }
\DoxyCodeLine{00059 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_ae772d750fa98fd997ee0f20e2e255f72}\label{namespaceaipy_1_1aipy_1_1utils_ae772d750fa98fd997ee0f20e2e255f72}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!deep\_activation@{deep\_activation}}
\index{deep\_activation@{deep\_activation}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{deep\_activation()}{deep\_activation()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+deep\+\_\+activation (\begin{DoxyParamCaption}\item[{}]{z }\end{DoxyParamCaption})}



Tan activation function. 


\begin{DoxyParams}{Parameters}
{\em z} & is the linear activation \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
tan(z) 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00332}{332}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00332 \textcolor{keyword}{def }deep\_activation(z):}
\DoxyCodeLine{00333     \textcolor{keywordflow}{return} np.tanh(z)}
\DoxyCodeLine{00334 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_ac1d2386f78b74fda92f3eedb0690688d}\label{namespaceaipy_1_1aipy_1_1utils_ac1d2386f78b74fda92f3eedb0690688d}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!deep\_compute\_cost@{deep\_compute\_cost}}
\index{deep\_compute\_cost@{deep\_compute\_cost}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{deep\_compute\_cost()}{deep\_compute\_cost()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+deep\+\_\+compute\+\_\+cost (\begin{DoxyParamCaption}\item[{}]{AL,  }\item[{}]{Y }\end{DoxyParamCaption})}



Implement the cost function defined by equation (7). 


\begin{DoxyParams}{Parameters}
{\em AL} & probability vector corresponding to your label predictions, shape (1, number of examples) \\
\hline
{\em Y} & true \char`\"{}label\char`\"{} vector (for example\+: containing 0 if non-\/cat, 1 if cat), shape (1, number of examples) \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
cost cross-\/entropy cost 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00065}{65}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00065 \textcolor{keyword}{def }deep\_compute\_cost(AL, Y):}
\DoxyCodeLine{00066     m = Y.shape[1]}
\DoxyCodeLine{00067     \textcolor{comment}{\# Compute loss from aL and y.}}
\DoxyCodeLine{00068     cost = -\/1.0/m * np.sum(np.multiply(Y, np.log(AL)) + \(\backslash\)}
\DoxyCodeLine{00069                            np.multiply((1-\/Y), np.log(1-\/AL)))}
\DoxyCodeLine{00070     \textcolor{comment}{\# To make sure your cost's shape is what we expect}}
\DoxyCodeLine{00071     \textcolor{comment}{\# (e.g. this turns [[17]] into 17).}}
\DoxyCodeLine{00072     cost = cost.squeeze()}
\DoxyCodeLine{00073     assert(cost.shape == ())}
\DoxyCodeLine{00074     }
\DoxyCodeLine{00075     \textcolor{keywordflow}{return} cost}
\DoxyCodeLine{00076 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_ac22409ca5bf4e2a84069b8832259aa2a}\label{namespaceaipy_1_1aipy_1_1utils_ac22409ca5bf4e2a84069b8832259aa2a}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!deep\_initialization@{deep\_initialization}}
\index{deep\_initialization@{deep\_initialization}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{deep\_initialization()}{deep\_initialization()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+deep\+\_\+initialization (\begin{DoxyParamCaption}\item[{}]{layer\+\_\+dims }\end{DoxyParamCaption})}



deep neural networks \# 

Randomly initialize weight, and bias parameters of given layer dimensions.


\begin{DoxyParams}{Parameters}
{\em layer\+\_\+dims} & python array (list) containing the dimensions of each layer in our network \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
parameters python dictionary containing your parameters \char`\"{}\+W1\char`\"{}, \char`\"{}b1\char`\"{}, ..., \char`\"{}\+W\+L\char`\"{}, \char`\"{}b\+L\char`\"{}\+: Wl weight matrix of shape(layer\+\_\+dims\mbox{[}l\mbox{]}, layer\+\_\+dims\mbox{[}l-\/1\mbox{]}) bl bias vector of shape (layer\+\_\+dims\mbox{[}l\mbox{]}, 1) 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00307}{307}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00307 \textcolor{keyword}{def }deep\_initialization(layer\_dims):}
\DoxyCodeLine{00308     np.random.seed(3)}
\DoxyCodeLine{00309     parameters = \{\}}
\DoxyCodeLine{00310     L = len(layer\_dims)}
\DoxyCodeLine{00311     \textcolor{keywordflow}{for} l \textcolor{keywordflow}{in} range(1, L):}
\DoxyCodeLine{00312         parameters[\textcolor{stringliteral}{'W'} + str(l)] = \(\backslash\)}
\DoxyCodeLine{00313             np.random.randn(layer\_dims[l], layer\_dims[l-\/1])*0.01}
\DoxyCodeLine{00314         parameters[\textcolor{stringliteral}{'b'} + str(l)] = \(\backslash\)}
\DoxyCodeLine{00315             np.zeros((layer\_dims[l], 1))*0.01}
\DoxyCodeLine{00316     \textcolor{keywordflow}{return} parameters}
\DoxyCodeLine{00317 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_abd0b296a51a42b8841de2e0ec584e21c}\label{namespaceaipy_1_1aipy_1_1utils_abd0b296a51a42b8841de2e0ec584e21c}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!deep\_linear@{deep\_linear}}
\index{deep\_linear@{deep\_linear}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{deep\_linear()}{deep\_linear()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+deep\+\_\+linear (\begin{DoxyParamCaption}\item[{}]{W,  }\item[{}]{A,  }\item[{}]{b }\end{DoxyParamCaption})}



linear activation function 


\begin{DoxyParams}{Parameters}
{\em W} & (n\+\_\+i,n\+\_\+\{i-\/1\}) weight matrix \\
\hline
{\em A} & (n\+\_\+\{i-\/1\}, m) input matrix \\
\hline
{\em b} & (1) bias \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
z (1,m) linear estimation 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00324}{324}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00324 \textcolor{keyword}{def }deep\_linear(W, A, b):}
\DoxyCodeLine{00325     z = np.dot(W, A) + b}
\DoxyCodeLine{00326     \textcolor{keywordflow}{return} z}
\DoxyCodeLine{00327 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a57e2d187ebc733e018a0dcf6a506af9a}\label{namespaceaipy_1_1aipy_1_1utils_a57e2d187ebc733e018a0dcf6a506af9a}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!deep\_linear\_activation\_backward@{deep\_linear\_activation\_backward}}
\index{deep\_linear\_activation\_backward@{deep\_linear\_activation\_backward}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{deep\_linear\_activation\_backward()}{deep\_linear\_activation\_backward()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+deep\+\_\+linear\+\_\+activation\+\_\+backward (\begin{DoxyParamCaption}\item[{}]{dA,  }\item[{}]{cache,  }\item[{}]{activation }\end{DoxyParamCaption})}



Implement the backward propagation for the L\+I\+N\+E\+A\+R-\/$>$A\+C\+T\+I\+V\+A\+T\+I\+ON layer. 


\begin{DoxyParams}{Parameters}
{\em dA} & post-\/activation gradient for current layer l \\
\hline
{\em cache} & tuple of values (linear\+\_\+cache, activation\+\_\+cache) we store for computing backward propagation efficiently \\
\hline
{\em activation} & the activation to be used in this layer, stored as a text string\+: \char`\"{}sigmoid\char`\"{} or \char`\"{}relu\char`\"{} \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
d\+A\+\_\+prev Gradient of the cost with respect to the activation (of the previous layer l-\/1), same shape as A\+\_\+prev. 

dW Gradient of the cost with respect to W (current layer l), same shape as W. 

db Gradient of the cost with respect to b (current layer l), same shape as b. 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00366}{366}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00366 \textcolor{keyword}{def }deep\_linear\_activation\_backward(dA, cache, activation):}
\DoxyCodeLine{00367     }
\DoxyCodeLine{00374     \textcolor{keyword}{def }deep\_linear\_backward(dZ, cache):}
\DoxyCodeLine{00375         A\_prev, W, b = cache}
\DoxyCodeLine{00376         m = A\_prev.shape[1]}
\DoxyCodeLine{00377 }
\DoxyCodeLine{00378         dW = 1.0/m *np.dot(dZ, A\_prev.T)}
\DoxyCodeLine{00379         db = 1.0/m * np.sum(dZ, axis=1, keepdims=\textcolor{keyword}{True})}
\DoxyCodeLine{00380         dA\_prev = np.dot(W.T, dZ)}
\DoxyCodeLine{00381     }
\DoxyCodeLine{00382         \textcolor{keyword}{assert} (dA\_prev.shape == A\_prev.shape)}
\DoxyCodeLine{00383         \textcolor{keyword}{assert} (dW.shape == W.shape)}
\DoxyCodeLine{00384         \textcolor{keyword}{assert} (db.shape == b.shape)}
\DoxyCodeLine{00385         }
\DoxyCodeLine{00386         \textcolor{keywordflow}{return} dA\_prev, dW, db}
\DoxyCodeLine{00387 }
\DoxyCodeLine{00388     linear\_cache, activation\_cache = cache}
\DoxyCodeLine{00389     \textcolor{keywordflow}{if} activation == \textcolor{stringliteral}{"relu"}:}
\DoxyCodeLine{00390         dZ = relu\_backward(dA, activation\_cache)}
\DoxyCodeLine{00391         dA\_prev, dW, db = deep\_linear\_backward(dZ, linear\_cache)}
\DoxyCodeLine{00392     \textcolor{keywordflow}{elif} activation == \textcolor{stringliteral}{"sigmoid"}:}
\DoxyCodeLine{00393         dZ = sigmoid\_backward(dA, activation\_cache)}
\DoxyCodeLine{00394         dA\_prev, dW, db = deep\_linear\_backward(dZ, linear\_cache)}
\DoxyCodeLine{00395     \textcolor{keywordflow}{return} dA\_prev, dW, db}

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a9053ccc44ee85c2b505ca8235b264ba4}\label{namespaceaipy_1_1aipy_1_1utils_a9053ccc44ee85c2b505ca8235b264ba4}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!deep\_linear\_activation\_forward@{deep\_linear\_activation\_forward}}
\index{deep\_linear\_activation\_forward@{deep\_linear\_activation\_forward}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{deep\_linear\_activation\_forward()}{deep\_linear\_activation\_forward()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+deep\+\_\+linear\+\_\+activation\+\_\+forward (\begin{DoxyParamCaption}\item[{}]{A\+\_\+prev,  }\item[{}]{W,  }\item[{}]{b,  }\item[{}]{activation }\end{DoxyParamCaption})}



Implement the forward propagation for the L\+I\+N\+E\+A\+R-\/$>$A\+C\+T\+I\+V\+A\+T\+I\+ON layer. 


\begin{DoxyParams}{Parameters}
{\em A\+\_\+prev} & activations from previous layer (or input data)\+: (size of previous layer, number of examples) \\
\hline
{\em W} & weights matrix\+: numpy array of shape (size of current layer, size of previous layer) \\
\hline
{\em b} & bias vector, numpy array of shape (size of the current layer, 1) \\
\hline
{\em activation} & the activation to be used in this layer, stored as a text string\+: \char`\"{}sigmoid\char`\"{} or \char`\"{}relu\char`\"{} \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A the output of the activation function, also called the post-\/activation value 

cache a python tuple containing \char`\"{}linear\+\_\+cache\char`\"{} and \char`\"{}activation\+\_\+cache\char`\"{}, stored for computing the backward pass efficiently 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00343}{343}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00343 \textcolor{keyword}{def }deep\_linear\_activation\_forward(A\_prev, W, b, activation):}
\DoxyCodeLine{00344     \textcolor{keywordflow}{if} activation == \textcolor{stringliteral}{"sigmoid"}:}
\DoxyCodeLine{00345         Z, linear\_cache = linear\_forward(A\_prev, W, b)}
\DoxyCodeLine{00346         \textcolor{comment}{\# TODO implement cached sigmoid}}
\DoxyCodeLine{00347         A, activation\_cache = deep\_sigmoid(Z)}
\DoxyCodeLine{00348     }
\DoxyCodeLine{00349     \textcolor{keywordflow}{elif} activation == \textcolor{stringliteral}{"relu"}:}
\DoxyCodeLine{00350         Z, linear\_cache = linear\_forward(A\_prev, W, b)\textcolor{comment}{\#}}
\DoxyCodeLine{00351         \textcolor{comment}{\#TODO implemnet cached relu}}
\DoxyCodeLine{00352         A, activation\_cache = relu(Z)}
\DoxyCodeLine{00353     }
\DoxyCodeLine{00354     \textcolor{keyword}{assert} (A.shape == (W.shape[0], A\_prev.shape[1]))}
\DoxyCodeLine{00355     cache = (linear\_cache, activation\_cache)}
\DoxyCodeLine{00356     \textcolor{keywordflow}{return} A, cache}
\DoxyCodeLine{00357 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a21734b28fb87a0c8494e865442d9b4ec}\label{namespaceaipy_1_1aipy_1_1utils_a21734b28fb87a0c8494e865442d9b4ec}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!deep\_sigmoid@{deep\_sigmoid}}
\index{deep\_sigmoid@{deep\_sigmoid}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{deep\_sigmoid()}{deep\_sigmoid()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+deep\+\_\+sigmoid (\begin{DoxyParamCaption}\item[{}]{Z }\end{DoxyParamCaption})}



Implements the sigmoid activation in numpy. 


\begin{DoxyParams}{Parameters}
{\em Z} & numpy array of any shape \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A output of sigmoid(z), same shape as Z 

cache returns Z as well, useful during backpropagation 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00041}{41}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00041 \textcolor{keyword}{def }deep\_sigmoid(Z):}
\DoxyCodeLine{00042     A = 1/(1+np.exp(-\/Z))}
\DoxyCodeLine{00043     cache = Z}
\DoxyCodeLine{00044     \textcolor{keywordflow}{return} A, cache}
\DoxyCodeLine{00045 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a87e9e817af292c366226f644168454a7}\label{namespaceaipy_1_1aipy_1_1utils_a87e9e817af292c366226f644168454a7}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!derive\_weight@{derive\_weight}}
\index{derive\_weight@{derive\_weight}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{derive\_weight()}{derive\_weight()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+derive\+\_\+weight (\begin{DoxyParamCaption}\item[{}]{X,  }\item[{}]{Y,  }\item[{}]{h }\end{DoxyParamCaption})}



compute d\+J/dw, and d\+J/db for logistic regression 


\begin{DoxyParams}{Parameters}
{\em X} & (n,m) input matrix \\
\hline
{\em Y} & (1,m) output vector \\
\hline
{\em h} & (1,m) activation vector \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
dw (n,1) vector derivate of cost function w.\+r.\+t w 

db (1) scalar derivate of cost function w.\+r.\+t b 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00084}{84}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00084 \textcolor{keyword}{def }derive\_weight(X, Y, h):    }
\DoxyCodeLine{00085     m=len(X[0])}
\DoxyCodeLine{00086     dw = 1.0/m * (np.dot(X, (h-\/Y).T))}
\DoxyCodeLine{00087     db = 1.0/m * np.sum(h-\/Y)}
\DoxyCodeLine{00088     \textcolor{keywordflow}{return} dw, db}
\DoxyCodeLine{00089 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a08062c4331864f05b91e3e6c385996a5}\label{namespaceaipy_1_1aipy_1_1utils_a08062c4331864f05b91e3e6c385996a5}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!forward\_propagation@{forward\_propagation}}
\index{forward\_propagation@{forward\_propagation}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{forward\_propagation()}{forward\_propagation()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+forward\+\_\+propagation (\begin{DoxyParamCaption}\item[{}]{X,  }\item[{}]{parameters }\end{DoxyParamCaption})}



Implement Forward propagation. 


\begin{DoxyParams}{Parameters}
{\em X} & input data of size (n\+\_\+x, m) \\
\hline
{\em parameters} & initialization return value \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A2 The sigmoid output of the second activation 

cache a dictionary containing \char`\"{}\+Z1\char`\"{}, \char`\"{}\+A1\char`\"{}, \char`\"{}\+Z2\char`\"{} and \char`\"{}\+A2\char`\"{} 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00276}{276}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00276 \textcolor{keyword}{def }forward\_propagation(X, parameters):}
\DoxyCodeLine{00277     W1 = parameters[\textcolor{stringliteral}{'W1'}]}
\DoxyCodeLine{00278     b1 = parameters[\textcolor{stringliteral}{'b1'}]}
\DoxyCodeLine{00279     W2 = parameters[\textcolor{stringliteral}{'W2'}]}
\DoxyCodeLine{00280     b2 = parameters[\textcolor{stringliteral}{'b2'}]}
\DoxyCodeLine{00281     }
\DoxyCodeLine{00282     Z1 = deep\_linear(W1, X, b1)}
\DoxyCodeLine{00283     A1 = deep\_activation(Z1)}
\DoxyCodeLine{00284     Z2 = deep\_linear(W2, A1, b2)}
\DoxyCodeLine{00285     A2 = sigmoid(Z2)}
\DoxyCodeLine{00286     }
\DoxyCodeLine{00287     assert(A2.shape == (1, X.shape[1]))}
\DoxyCodeLine{00288 }
\DoxyCodeLine{00289     cache = \{\textcolor{stringliteral}{"Z1"}: Z1,}
\DoxyCodeLine{00290              \textcolor{stringliteral}{"A1"}: A1,}
\DoxyCodeLine{00291              \textcolor{stringliteral}{"Z2"}: Z2,}
\DoxyCodeLine{00292              \textcolor{stringliteral}{"A2"}: A2\}}
\DoxyCodeLine{00293     \textcolor{keywordflow}{return} A2, cache}
\DoxyCodeLine{00294 }
\DoxyCodeLine{00295 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a96dbd24dcc5f813264a1bc91be208946}\label{namespaceaipy_1_1aipy_1_1utils_a96dbd24dcc5f813264a1bc91be208946}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!gradient\_descent@{gradient\_descent}}
\index{gradient\_descent@{gradient\_descent}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{gradient\_descent()}{gradient\_descent()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+gradient\+\_\+descent (\begin{DoxyParamCaption}\item[{}]{w,  }\item[{}]{b,  }\item[{}]{X,  }\item[{}]{Y,  }\item[{}]{num\+\_\+iterations = {\ttfamily 1000},  }\item[{}]{alpha = {\ttfamily 0.001},  }\item[{}]{print\+\_\+cost = {\ttfamily False} }\end{DoxyParamCaption})}



This function optimizes w and b by running a gradient descent algorithm. 


\begin{DoxyParams}{Parameters}
{\em w} & weights, a numpy array of size (n,1) \\
\hline
{\em b} & bias, a scalar \\
\hline
{\em X} & data of shape (n,m) \\
\hline
{\em Y} & true \char`\"{}label\char`\"{} vector (containing 0 if non-\/cat, 1 if cat), of shape (1, number of examples) \\
\hline
{\em num\+\_\+iterations} & number of iterations of the optimization loop \\
\hline
{\em learning\+\_\+rate} & learning rate of the gradient descent update rule \\
\hline
{\em print\+\_\+cost} & True to print the loss every 100 steps \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
params dictionary containing the weights w and bias b 

grads dictionary containing the gradients of the weights and bias with respect to the cost function 

costs list of all the costs computed during the optimization, this will be used to plot the learning curve. 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00154}{154}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00154 \textcolor{keyword}{def }gradient\_descent(w, b, X, Y, num\_iterations=1000, alpha=0.001, print\_cost = False):}
\DoxyCodeLine{00155     costs = []}
\DoxyCodeLine{00156     \textcolor{keywordflow}{for} i \textcolor{keywordflow}{in} range(num\_iterations):}
\DoxyCodeLine{00157         grads, cost = propagate(w, b, X, Y)}
\DoxyCodeLine{00158         dw = grads[\textcolor{stringliteral}{"dw"}]}
\DoxyCodeLine{00159         db = grads[\textcolor{stringliteral}{"db"}]}
\DoxyCodeLine{00160         w,b = update\_weight(w, dw, b, db, alpha)}
\DoxyCodeLine{00161         \textcolor{comment}{\# track cost}}
\DoxyCodeLine{00162         \textcolor{keywordflow}{if} i \% 100 == 0:}
\DoxyCodeLine{00163             costs.append(cost)}
\DoxyCodeLine{00164         \textcolor{comment}{\# Print the cost every 100 training iterations}}
\DoxyCodeLine{00165         \textcolor{keywordflow}{if} print\_cost \textcolor{keywordflow}{and} i \% 100 == 0:}
\DoxyCodeLine{00166             \textcolor{keywordflow}{print} (\textcolor{stringliteral}{"Cost after iteration \%i: \%f"} \%(i, cost))}
\DoxyCodeLine{00167     }
\DoxyCodeLine{00168     params = \{\textcolor{stringliteral}{"w"}: w,}
\DoxyCodeLine{00169               \textcolor{stringliteral}{"b"}: b\}}
\DoxyCodeLine{00170     }
\DoxyCodeLine{00171     grads = \{\textcolor{stringliteral}{"dw"}: dw,}
\DoxyCodeLine{00172              \textcolor{stringliteral}{"db"}: db\}}
\DoxyCodeLine{00173     }
\DoxyCodeLine{00174     \textcolor{keywordflow}{return} params, grads, costs}
\DoxyCodeLine{00175 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_ac5ddc42640b585e150188f5afba6dfe7}\label{namespaceaipy_1_1aipy_1_1utils_ac5ddc42640b585e150188f5afba6dfe7}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!initialize\_parameters@{initialize\_parameters}}
\index{initialize\_parameters@{initialize\_parameters}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{initialize\_parameters()}{initialize\_parameters()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+initialize\+\_\+parameters (\begin{DoxyParamCaption}\item[{}]{n\+\_\+x,  }\item[{}]{n\+\_\+h,  }\item[{}]{n\+\_\+y }\end{DoxyParamCaption})}



Randomly initialize the parameters W, b. 


\begin{DoxyParams}{Parameters}
{\em n\+\_\+x} & size of the input layer \\
\hline
{\em n\+\_\+h} & size of the hidden layer \\
\hline
{\em n\+\_\+y} & size of the output layer \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
params python dictionary containing your parameters\+: W1 -- weight matrix of shape (n\+\_\+h, n\+\_\+x) b1 -- bias vector of shape (n\+\_\+h, 1) W2 -- weight matrix of shape (n\+\_\+y, n\+\_\+h) b2 -- bias vector of shape (n\+\_\+y, 1) 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00242}{242}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00242 \textcolor{keyword}{def }initialize\_parameters(n\_x, n\_h, n\_y):}
\DoxyCodeLine{00243     np.random.seed(2) }
\DoxyCodeLine{00244     W1 = np.random.random((n\_h, n\_x))*0.01}
\DoxyCodeLine{00245     b1 = np.zeros((n\_h, 1))*0.01}
\DoxyCodeLine{00246     W2 = np.random.randn(n\_y, n\_h)*0.01}
\DoxyCodeLine{00247     b2 = np.zeros((n\_y, 1))*0.01    }
\DoxyCodeLine{00248     \textcolor{keyword}{assert} (W1.shape == (n\_h, n\_x))}
\DoxyCodeLine{00249     \textcolor{keyword}{assert} (b1.shape == (n\_h, 1))}
\DoxyCodeLine{00250     \textcolor{keyword}{assert} (W2.shape == (n\_y, n\_h))}
\DoxyCodeLine{00251     \textcolor{keyword}{assert} (b2.shape == (n\_y, 1))}
\DoxyCodeLine{00252     parameters = \{\textcolor{stringliteral}{"W1"}: W1,}
\DoxyCodeLine{00253                   \textcolor{stringliteral}{"b1"}: b1,}
\DoxyCodeLine{00254                   \textcolor{stringliteral}{"W2"}: W2,}
\DoxyCodeLine{00255                   \textcolor{stringliteral}{"b2"}: b2\}}
\DoxyCodeLine{00256     \textcolor{keywordflow}{return} parameters}
\DoxyCodeLine{00257 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_adc552a252a7228b685a589a2778c9229}\label{namespaceaipy_1_1aipy_1_1utils_adc552a252a7228b685a589a2778c9229}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!initialize\_with\_zeros@{initialize\_with\_zeros}}
\index{initialize\_with\_zeros@{initialize\_with\_zeros}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{initialize\_with\_zeros()}{initialize\_with\_zeros()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+initialize\+\_\+with\+\_\+zeros (\begin{DoxyParamCaption}\item[{}]{dim }\end{DoxyParamCaption})}



This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. 


\begin{DoxyParams}{Parameters}
{\em dim} & size of the w vector we want (or number of parameters in this case) \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
w initialized vector of shape (dim, 1) 

b initialized scalar (corresponds to the bias) 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00013}{13}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00013 \textcolor{keyword}{def }initialize\_with\_zeros(dim):}
\DoxyCodeLine{00014     w = np.zeros((dim, 1))}
\DoxyCodeLine{00015     b = 0}
\DoxyCodeLine{00016     \textcolor{keywordflow}{return} w, b}
\DoxyCodeLine{00017 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_aa064d8aa342cee14bd78510b0e607860}\label{namespaceaipy_1_1aipy_1_1utils_aa064d8aa342cee14bd78510b0e607860}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!linear@{linear}}
\index{linear@{linear}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{linear()}{linear()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+linear (\begin{DoxyParamCaption}\item[{}]{w,  }\item[{}]{X,  }\item[{}]{b }\end{DoxyParamCaption})}



linear activation function 


\begin{DoxyParams}{Parameters}
{\em w} & (m,1) weight matrix \\
\hline
{\em X} & (m,n) input matrix \\
\hline
{\em b} & (1) bias \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
z (1,m) linear estimation 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00024}{24}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00024 \textcolor{keyword}{def }linear(w, X, b):}
\DoxyCodeLine{00025     z = np.dot(w.T, X) + b}
\DoxyCodeLine{00026     \textcolor{keywordflow}{return} z}
\DoxyCodeLine{00027 }
\DoxyCodeLine{00028 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_afbac86750318632038fd04f8ad723177}\label{namespaceaipy_1_1aipy_1_1utils_afbac86750318632038fd04f8ad723177}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!linear\_forward@{linear\_forward}}
\index{linear\_forward@{linear\_forward}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{linear\_forward()}{linear\_forward()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+linear\+\_\+forward (\begin{DoxyParamCaption}\item[{}]{A,  }\item[{}]{W,  }\item[{}]{b }\end{DoxyParamCaption})}



Implement the linear part of a layer\textquotesingle{}s forward propagation. 


\begin{DoxyParams}{Parameters}
{\em A} & activations from previous layer (or input data)\+: (size of previous layer, number of examples) \\
\hline
{\em W} & weights matrix\+: numpy array of shape (size of current layer, size of previous layer) \\
\hline
{\em b} & bias vector, numpy array of shape (size of the current layer, 1) \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Z the input of the activation function, also called pre-\/activation parameter 

cache a python tuple containing \char`\"{}\+A\char`\"{}, \char`\"{}\+W\char`\"{} and \char`\"{}b\char`\"{} ; stored for computing the backward pass efficiently 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00265}{265}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00265 \textcolor{keyword}{def }linear\_forward(A, W, b):}
\DoxyCodeLine{00266     Z = deep\_linear(W, A, b)    }
\DoxyCodeLine{00267     cache = (A, W, b)}
\DoxyCodeLine{00268     \textcolor{keywordflow}{return} Z, cache}
\DoxyCodeLine{00269 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a88d9dbe6880b5c7ce1978a290b21b0db}\label{namespaceaipy_1_1aipy_1_1utils_a88d9dbe6880b5c7ce1978a290b21b0db}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!predict@{predict}}
\index{predict@{predict}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{predict()}{predict()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+predict (\begin{DoxyParamCaption}\item[{}]{w,  }\item[{}]{b,  }\item[{}]{X }\end{DoxyParamCaption})}



Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) 


\begin{DoxyParams}{Parameters}
{\em w} & weights, a numpy array of size (n, 1) \\
\hline
{\em b} & bias, a scalar \\
\hline
{\em X} & data of size (n,m) \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Y\+\_\+prediction a numpy array (vector) containing all predictions (0/1) for the examples in X 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00182}{182}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00182 \textcolor{keyword}{def }predict(w, b, X):}
\DoxyCodeLine{00183     m = X.shape[1]}
\DoxyCodeLine{00184     Y\_prediction = np.zeros((1,m))}
\DoxyCodeLine{00185     w = w.reshape(X.shape[0], 1)}
\DoxyCodeLine{00186     Z = linear(w, X, b)}
\DoxyCodeLine{00187     A = sigmoid(Z)}
\DoxyCodeLine{00188     \textcolor{comment}{\#TODO replace this with pandas apply function}}
\DoxyCodeLine{00189     \textcolor{comment}{\#RELU-\/like activation}}
\DoxyCodeLine{00190     \textcolor{keywordflow}{for} i \textcolor{keywordflow}{in} range(A.shape[1]):}
\DoxyCodeLine{00191         Y\_prediction[0][i] = 1 \textcolor{keywordflow}{if} A[0][i]>0.5 \textcolor{keywordflow}{else} 0}
\DoxyCodeLine{00192     assert(Y\_prediction.shape == (1, m))}
\DoxyCodeLine{00193     \textcolor{keywordflow}{return} Y\_prediction}
\DoxyCodeLine{00194 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a58e2fccbc2a5b25f8f30607629ed38ae}\label{namespaceaipy_1_1aipy_1_1utils_a58e2fccbc2a5b25f8f30607629ed38ae}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!propagate@{propagate}}
\index{propagate@{propagate}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{propagate()}{propagate()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+propagate (\begin{DoxyParamCaption}\item[{}]{w,  }\item[{}]{b,  }\item[{}]{X,  }\item[{}]{Y }\end{DoxyParamCaption})}



Implement Forward, and Backward propagation, the cost function and its gradient . 


\begin{DoxyParams}{Parameters}
{\em w} & weights, a numpy array of size (n,1) \\
\hline
{\em b} & bias, a scalar \\
\hline
{\em X} & data of size (n,m) \\
\hline
{\em Y} & true \char`\"{}label\char`\"{} vector \{0,1\} (1,m) \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
cost negative log-\/likelihood cost for logistic regression 

dw gradient of the loss with respect to w, thus same shape as w 

db gradient of the loss with respect to b, thus same shape as b 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00129}{129}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00129 \textcolor{keyword}{def }propagate(w, b, X, Y):}
\DoxyCodeLine{00130     m = X.shape[1]}
\DoxyCodeLine{00131     z = linear(w, X, b)}
\DoxyCodeLine{00132     h = sigmoid(z)}
\DoxyCodeLine{00133     cost = deep\_compute\_cost(Y, h)}
\DoxyCodeLine{00134     dw, db = derive\_weight(X, Y, h)}
\DoxyCodeLine{00135     assert(dw.shape == w.shape)}
\DoxyCodeLine{00136     assert(db.dtype == float)}
\DoxyCodeLine{00137     assert(cost.shape == ())}
\DoxyCodeLine{00138     grads = \{\textcolor{stringliteral}{"dw"}: dw,}
\DoxyCodeLine{00139              \textcolor{stringliteral}{"db"}: db\}}
\DoxyCodeLine{00140     \textcolor{keywordflow}{return} grads, cost}
\DoxyCodeLine{00141 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a314bb57a2c296337ea2f273c46f4e7e7}\label{namespaceaipy_1_1aipy_1_1utils_a314bb57a2c296337ea2f273c46f4e7e7}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!relu@{relu}}
\index{relu@{relu}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{relu()}{relu()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+relu (\begin{DoxyParamCaption}\item[{}]{Z }\end{DoxyParamCaption})}



Implement the R\+E\+LU function. 


\begin{DoxyParams}{Parameters}
{\em Z} & Output of the linear layer, of any shape \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A Post-\/activation parameter, of the same shape as Z 

cache a python dictionary containing \char`\"{}\+A\char`\"{} ; stored for computing the backward pass efficiently 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00200}{200}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00200 \textcolor{keyword}{def }relu(Z):}
\DoxyCodeLine{00201     A = np.maximum(0,Z)}
\DoxyCodeLine{00202     assert(A.shape == Z.shape)}
\DoxyCodeLine{00203     cache = Z }
\DoxyCodeLine{00204     \textcolor{keywordflow}{return} A, cache}
\DoxyCodeLine{00205 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_ae93689ee55421e957e55192016219da0}\label{namespaceaipy_1_1aipy_1_1utils_ae93689ee55421e957e55192016219da0}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!relu\_backward@{relu\_backward}}
\index{relu\_backward@{relu\_backward}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{relu\_backward()}{relu\_backward()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+relu\+\_\+backward (\begin{DoxyParamCaption}\item[{}]{dA,  }\item[{}]{cache }\end{DoxyParamCaption})}



Implement the backward propagation for a single R\+E\+LU unit. 


\begin{DoxyParams}{Parameters}
{\em dA} & post-\/activation gradient, of any shape \\
\hline
{\em cache} & \textquotesingle{}Z\textquotesingle{} where we store for computing backward propagation efficiently \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
dZ Gradient of the cost with respect to Z 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00211}{211}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00211 \textcolor{keyword}{def }relu\_backward(dA, cache):   }
\DoxyCodeLine{00212     Z = cache}
\DoxyCodeLine{00213     \textcolor{comment}{\# just converting dz to a correct object.}}
\DoxyCodeLine{00214     dZ = np.array(dA, copy=\textcolor{keyword}{True}) }
\DoxyCodeLine{00215     \textcolor{comment}{\# When z < 0, you should set dz to 0 as well. }}
\DoxyCodeLine{00216     dZ[Z <= 0] = 0    }
\DoxyCodeLine{00217     \textcolor{keyword}{assert} (dZ.shape == Z.shape)}
\DoxyCodeLine{00218     \textcolor{keywordflow}{return} dZ}
\DoxyCodeLine{00219 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_ab953b6a76b3d91de0f6b42d9d6d8b58b}\label{namespaceaipy_1_1aipy_1_1utils_ab953b6a76b3d91de0f6b42d9d6d8b58b}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!sigmoid@{sigmoid}}
\index{sigmoid@{sigmoid}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{sigmoid()}{sigmoid()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+sigmoid (\begin{DoxyParamCaption}\item[{}]{z }\end{DoxyParamCaption})}



sigmoid activation 


\begin{DoxyParams}{Parameters}
{\em z} & is the input (can be a scalar or an array) \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
h the sigmoid of z 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00033}{33}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00033 \textcolor{keyword}{def }sigmoid(z):}
\DoxyCodeLine{00034     \textcolor{keywordflow}{return} 1/(1+np.exp(-\/1*z))}
\DoxyCodeLine{00035 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_aec2b9988afa351577eb52f731029fc89}\label{namespaceaipy_1_1aipy_1_1utils_aec2b9988afa351577eb52f731029fc89}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!sigmoid\_backward@{sigmoid\_backward}}
\index{sigmoid\_backward@{sigmoid\_backward}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{sigmoid\_backward()}{sigmoid\_backward()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+sigmoid\+\_\+backward (\begin{DoxyParamCaption}\item[{}]{dA,  }\item[{}]{cache }\end{DoxyParamCaption})}



Implement the backward propagation for a single S\+I\+G\+M\+O\+ID unit. 


\begin{DoxyParams}{Parameters}
{\em dA} & post-\/activation gradient, of any shape \\
\hline
{\em cache} & \textquotesingle{}Z\textquotesingle{} where we store for computing backward propagation efficiently \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
dZ Gradient of the cost with respect to Z 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00225}{225}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00225 \textcolor{keyword}{def }sigmoid\_backward(dA, cache):}
\DoxyCodeLine{00226     Z = cache    }
\DoxyCodeLine{00227     s = 1/(1+np.exp(-\/Z))}
\DoxyCodeLine{00228     dZ = dA * s * (1-\/s)}
\DoxyCodeLine{00229     \textcolor{keyword}{assert} (dZ.shape == Z.shape)}
\DoxyCodeLine{00230     \textcolor{keywordflow}{return} dZ}
\DoxyCodeLine{00231 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_af4b8d65cdcdd5476f1fd2c871464c391}\label{namespaceaipy_1_1aipy_1_1utils_af4b8d65cdcdd5476f1fd2c871464c391}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!update\_parameters@{update\_parameters}}
\index{update\_parameters@{update\_parameters}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{update\_parameters()}{update\_parameters()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+update\+\_\+parameters (\begin{DoxyParamCaption}\item[{}]{parameters,  }\item[{}]{grads,  }\item[{}]{learning\+\_\+rate }\end{DoxyParamCaption})}



Update parameters using gradient descent. 


\begin{DoxyParams}{Parameters}
{\em parameters} & python dictionary containing your parameters \\
\hline
{\em grads} & python dictionary containing your gradients, output of L\+\_\+model\+\_\+backward. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
updated parameters. 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00108}{108}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00108 \textcolor{keyword}{def }update\_parameters(parameters, grads, learning\_rate):}
\DoxyCodeLine{00109     L = len(parameters) // 2 }
\DoxyCodeLine{00110     \textcolor{comment}{\# Update rule for each parameter. Use a for loop.}}
\DoxyCodeLine{00111     \textcolor{keywordflow}{for} l \textcolor{keywordflow}{in} range(L):}
\DoxyCodeLine{00112         w=parameters[\textcolor{stringliteral}{"W"} + str(l+1)]}
\DoxyCodeLine{00113         b=parameters[\textcolor{stringliteral}{"b"} + str(l+1)]}
\DoxyCodeLine{00114         dw=learning\_rate*grads[\textcolor{stringliteral}{'dW'}+str(l+1)]}
\DoxyCodeLine{00115         db=learning\_rate*grads[\textcolor{stringliteral}{'db'}+str(l+1)]}
\DoxyCodeLine{00116         parameters[\textcolor{stringliteral}{"W"} + str(l+1)], parameters[\textcolor{stringliteral}{"b"} + str(l+1)]=\(\backslash\)}
\DoxyCodeLine{00117             update\_weight(w, dw, b, db, learning\_rate)}
\DoxyCodeLine{00118     \textcolor{keywordflow}{return} parameters}
\DoxyCodeLine{00119 }

\end{DoxyCode}
\mbox{\Hypertarget{namespaceaipy_1_1aipy_1_1utils_a757ce3a23c10fbc489f586d772381d68}\label{namespaceaipy_1_1aipy_1_1utils_a757ce3a23c10fbc489f586d772381d68}} 
\index{aipy.aipy.utils@{aipy.aipy.utils}!update\_weight@{update\_weight}}
\index{update\_weight@{update\_weight}!aipy.aipy.utils@{aipy.aipy.utils}}
\doxysubsubsection{\texorpdfstring{update\_weight()}{update\_weight()}}
{\footnotesize\ttfamily def aipy.\+aipy.\+utils.\+update\+\_\+weight (\begin{DoxyParamCaption}\item[{}]{w,  }\item[{}]{dw,  }\item[{}]{b,  }\item[{}]{db,  }\item[{}]{alpha }\end{DoxyParamCaption})}



Update weight values with single gradient descent step. 


\begin{DoxyParams}{Parameters}
{\em w} & (n,1) weight vector . \\
\hline
{\em dw} & (n,1) weight vector of d\+J/dw. \\
\hline
{\em b} & scalar bias. \\
\hline
{\em db} & scalar d\+J/db. \\
\hline
{\em alpha} & scalar is the learning reate. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
tuple of new (w,b). 
\end{DoxyReturn}


Definition at line \mbox{\hyperlink{utils_8py_source_l00098}{98}} of file \mbox{\hyperlink{utils_8py_source}{utils.\+py}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{00098 \textcolor{keyword}{def }update\_weight(w, dw, b, db, alpha):}
\DoxyCodeLine{00099     w = w -\/ alpha * dw}
\DoxyCodeLine{00100     b = b -\/ alpha * db}
\DoxyCodeLine{00101     \textcolor{keywordflow}{return} w, b}
\DoxyCodeLine{00102  }

\end{DoxyCode}
